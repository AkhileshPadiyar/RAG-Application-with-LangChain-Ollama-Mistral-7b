# ğŸ§  LangChain RAG App using Mistral:7B (via Ollama)

A lightweight, local Retrieval-Augmented Generation (RAG) application powered by [LangChain](https://www.langchain.com/) and the open-source [Mistral:7B](https://ollama.com/library/mistral) language model, served locally using [Ollama](https://ollama.com/). Built using the Cursor IDE, this app allows users to query content from a `sample.txt` file, with accurate, context-rich answers generated by the model.

---

## ğŸš€ Features

- âš¡ **Local-first** RAG pipeline â€” no API key or cloud dependency
- ğŸ§± Powered by **LangChain** for modular retrieval + reasoning
- ğŸ§  Uses **Mistral:7B** model through **Ollama** backend
- ğŸ“„ Processes and retrieves relevant chunks from `sample.txt`
- ğŸ¤– Reasoned responses for natural-language queries

---

## ğŸ› ï¸ Tech Stack

- **LangChain** â€” orchestration of retrieval + generation
- **Ollama** â€” local LLM model runner (Mistral 7B)
- **Python** â€” core logic and execution
- **Cursor IDE** â€” lightweight development environment

---
