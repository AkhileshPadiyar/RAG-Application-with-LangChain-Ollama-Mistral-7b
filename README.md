# 🧠 LangChain RAG App using Mistral:7B (via Ollama)

A lightweight, local Retrieval-Augmented Generation (RAG) application powered by [LangChain](https://www.langchain.com/) and the open-source [Mistral:7B](https://ollama.com/library/mistral) language model, served locally using [Ollama](https://ollama.com/). Built using the Cursor IDE, this app allows users to query content from a `sample.txt` file, with accurate, context-rich answers generated by the model.

---

## 🚀 Features

- ⚡ **Local-first** RAG pipeline — no API key or cloud dependency
- 🧱 Powered by **LangChain** for modular retrieval + reasoning
- 🧠 Uses **Mistral:7B** model through **Ollama** backend
- 📄 Processes and retrieves relevant chunks from `sample.txt`
- 🤖 Reasoned responses for natural-language queries

---

## 🛠️ Tech Stack

- **LangChain** — orchestration of retrieval + generation
- **Ollama** — local LLM model runner (Mistral 7B)
- **Python** — core logic and execution
- **Cursor IDE** — lightweight development environment

---
